{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "if \"/opt/condaenv/anaconda2/envs/sentimentInduction\" not in sys.path:\n",
    "    sys.path.append(\"/opt/condaenv/anaconda2/envs/sentimentInduction\")\n",
    "if \"/opt/condaenv/anaconda2/envs/sentimentInduction/lib/python2.7/site-packages\" not in sys.path:\n",
    "    sys.path.append(\"/opt/condaenv/anaconda2/envs/sentimentInduction/lib/python2.7/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from socialconfig import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/condaenv/anaconda2/envs/sentimentInduction/socialsentRun/CollectPolaritiesForAllModelParams\n",
      "/opt/condaenv/anaconda2/envs/sentimentInduction/socialsentRun/Top30PosNegWords\n"
     ]
    }
   ],
   "source": [
    "SAVE_ALL_MODEL_PARAMS_COLLECTIVE_POLARITIES_DIR = config.get(\"SAVE_ALL_MODEL_PARAMS_COLLECTIVE_POLARITIES_DIR\")\n",
    "SAVE_DICTIONARY_DIR = config.get(\"SAVE_DICTIONARY_DIR\")\n",
    "methods = ['densify','random_walk','label_propagate_probabilistic','label_propagate_continuous','graph_propagate']\n",
    "print(SAVE_ALL_MODEL_PARAMS_COLLECTIVE_POLARITIES_DIR)\n",
    "SAVE_TOP30_POS_NEG_WORDS_ALL_MODELS_DIR = \"/opt/condaenv/anaconda2/envs/sentimentInduction/socialsentRun/Top30PosNegWords\"\n",
    "print(SAVE_TOP30_POS_NEG_WORDS_ALL_MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Mean Polarities and Saving.....\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing as prep\n",
    "\n",
    "print(\"Calculating Mean Polarities and Saving.....\")\n",
    "for category_entity in os.listdir(SAVE_ALL_MODEL_PARAMS_COLLECTIVE_POLARITIES_DIR):\n",
    "    category_entity_abs_path = os.path.join(SAVE_ALL_MODEL_PARAMS_COLLECTIVE_POLARITIES_DIR,category_entity)\n",
    "    if category_entity[0] != \".\" and os.path.isdir(category_entity_abs_path):\n",
    "        #dflist[category_entity] = {}\n",
    "        #print(\"For category: {c}\".format(c=category_entity))\n",
    "        for vocab_type_entity in os.listdir(category_entity_abs_path):\n",
    "            vocab_type_abs_path = os.path.join(category_entity_abs_path,vocab_type_entity)\n",
    "            if vocab_type_entity[0] != \".\" and os.path.isdir(vocab_type_abs_path):\n",
    "                # print(\"For vocab_type: {c}\".format(c=vocab_type_entity))\n",
    "                \n",
    "                for induction_method_type_entity in os.listdir(vocab_type_abs_path):\n",
    "                    induction_method_abs_path = os.path.join(vocab_type_abs_path,induction_method_type_entity)\n",
    "                    if induction_method_type_entity[0] != \".\" and os.path.isdir(induction_method_abs_path):\n",
    "                        # print(\"For induction method: {c}\".format(c=induction_method_type_entity))\n",
    "                        #dflist[category_entity][vocab_type_entity][induction_method_type_entity] = None\n",
    "                        for polarities_type_file in os.listdir(induction_method_abs_path):\n",
    "                            if \"all_polarities\" in polarities_type_file:\n",
    "                                polarities_file_abs_path = os.path.join(induction_method_abs_path,polarities_type_file)\n",
    "                                #print(\"Reading Polarity File..\")\n",
    "                                df = None\n",
    "                                df = pd.read_json(path_or_buf=polarities_file_abs_path,orient=\"records\",lines=True)\n",
    "                                \n",
    "                                #print(df.head())\n",
    "                                tokens = df['token']\n",
    "                                #print(df.head())\n",
    "                                df = df.drop(['token'],axis=1)\n",
    "                                #print(df.columns)\n",
    "                                #df -= df.min()\n",
    "                                #df /= df.max()\n",
    "                                df_std = (df - df.min()) / (df.max() - df.min())\n",
    "                                df = df_std * (1.0 - -1.0) + -1.0\n",
    "                                #df = prep.minmax_scale(df,feature_range=(-1,1))\n",
    "                                df.insert(0, 'token', tokens)\n",
    "                                #print(df.head())\n",
    "                                mean_pol = df.mean(axis=1)\n",
    "                                var_pol = df.var(axis=1)\n",
    "                                df['mean_pol'] = mean_pol\n",
    "                                df['var_pol'] = var_pol\n",
    "                                df_final = df[['token','mean_pol','var_pol']]\n",
    "                                del df\n",
    "                                df_final.sort_values(by=['mean_pol'],inplace=True,ascending=False)\n",
    "                                write_file = polarities_type_file.replace(\"all_polarities\",\"mean_polarities\")\n",
    "                                #print(\"Writing to \" + write_file)\n",
    "                                df_final.to_json(os.path.join(induction_method_abs_path,write_file),orient='records',lines=True)\n",
    "                                #print(df_final.head())\n",
    "                                #raise\n",
    "                                del df_final\n",
    "                            else:\n",
    "                                pass\n",
    "                                #os.remove(os.path.join(induction_method_abs_path,polarities_type_file))\n",
    "print(\"Done\")\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting positive, negative words\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Get positive, negative words\n",
    "print(\"Getting positive, negative words\")\n",
    "from socialsentRun.socialconfig import FOOD_POSITIVE_SEEDS,FOOD_NEGATIVE_SEEDS,BARS_POSITIVE_SEEDS,BARS_NEGATIVE_SEEDS,LEISURE_POSITIVE_SEEDS,LEISURE_NEGATIVE_SEEDS,GROOMING_POSITIVE_SEEDS,GROOMING_NEGATIVE_SEEDS,HEALTH_POSITIVE_SEEDS,HEALTH_NEGATIVE_SEEDS,LEARN_POSITIVE_SEEDS,LEARN_NEGATIVE_SEEDS,SHOPPING_POSITIVE_SEEDS,SHOPPING_NEGATIVE_SEEDS,PLANNING_POSITIVE_SEEDS,PLANNING_NEGATIVE_SEEDS,SERVICES_POSITIVE_SEEDS,SERVICES_NEGATIVE_SEEDS,SPORTS_POSITIVE_SEEDS,SPORTS_NEGATIVE_SEEDS,MUNICIPAL_POSITIVE_SEEDS,MUNICIPAL_NEGATIVE_SEEDS\n",
    "del dflist\n",
    "if category_entity == \"food\":\n",
    "    FOOD_POSITIVE_SEEDS.extend(FOOD_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = FOOD_POSITIVE_SEEDS\n",
    "elif category_entity == \"bars\":\n",
    "    BARS_POSITIVE_SEEDS.extend(BARS_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = BARS_POSITIVE_SEEDS\n",
    "elif category_entity == \"leisure\":\n",
    "    LEISURE_POSITIVE_SEEDS.extend(LEISURE_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = LEISURE_POSITIVE_SEEDS\n",
    "elif category_entity == \"grooming\":\n",
    "    GROOMING_POSITIVE_SEEDS.extend(GROOMING_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = GROOMING_POSITIVE_SEEDS\n",
    "elif category_entity == \"health\":\n",
    "    HEALTH_POSITIVE_SEEDS.extend(HEALTH_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = HEALTH_POSITIVE_SEEDS\n",
    "elif category_entity == \"learn\":\n",
    "    LEARN_POSITIVE_SEEDS.extend(LEARN_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = LEARN_POSITIVE_SEEDS\n",
    "elif category_entity == \"municipal\":\n",
    "    MUNICIPAL_POSITIVE_SEEDS.extend(MUNICIPAL_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = MUNICIPAL_POSITIVE_SEEDS\n",
    "elif category_entity == \"sports\":\n",
    "    SPORTS_POSITIVE_SEEDS.extend(SPORTS_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = SPORTS_POSITIVE_SEEDS\n",
    "elif category_entity == \"planning\":\n",
    "    PLANNING_POSITIVE_SEEDS.extend(PLANNING_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = PLANNING_POSITIVE_SEEDS\n",
    "elif category_entity == \"services\":\n",
    "    SERVICES_POSITIVE_SEEDS.extend(SERVICES_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = SERVICES_POSITIVE_SEEDS\n",
    "elif category_entity == \"shopping\":\n",
    "    SHOPPING_POSITIVE_SEEDS.extend(SHOPPING_NEGATIVE_SEEDS)\n",
    "    exclude_tokens = SHOPPING_POSITIVE_SEEDS\n",
    "else:\n",
    "    exclude_tokens = []\n",
    "dflist = {}\n",
    "for category_entity in os.listdir(SAVE_ALL_MODEL_PARAMS_COLLECTIVE_POLARITIES_DIR):\n",
    "    category_entity_abs_path = os.path.join(SAVE_ALL_MODEL_PARAMS_COLLECTIVE_POLARITIES_DIR,category_entity)\n",
    "    if category_entity[0] != \".\" and os.path.isdir(category_entity_abs_path):\n",
    "        dflist[category_entity] = {}\n",
    "        for vocab_type_entity in os.listdir(category_entity_abs_path):\n",
    "            vocab_type_abs_path = os.path.join(category_entity_abs_path,vocab_type_entity)\n",
    "            if vocab_type_entity[0] != \".\" and os.path.isdir(vocab_type_abs_path):\n",
    "                dflist[category_entity][vocab_type_entity] = {}\n",
    "                for induction_method_type_entity in os.listdir(vocab_type_abs_path):\n",
    "                    induction_method_abs_path = os.path.join(vocab_type_abs_path,induction_method_type_entity)\n",
    "                    if induction_method_type_entity[0] != \".\" and os.path.isdir(induction_method_abs_path):\n",
    "                        dflist[category_entity][vocab_type_entity][induction_method_type_entity] = {}\n",
    "                        for polarities_type_file in os.listdir(induction_method_abs_path):\n",
    "                            polarities_file_abs_path = os.path.join(induction_method_abs_path,polarities_type_file)\n",
    "                            if polarities_type_file[0] != \".\" and \"mean\" in polarities_type_file:\n",
    "                                df = pd.read_json(path_or_buf=polarities_file_abs_path,orient=\"records\",lines=True)\n",
    "                                #df.sort_values(by=['mean_pol'],inplace=True)\n",
    "                                df = df[df.token.notnull() & ~df.token.str.contains(\"\\d\") & ~df.token.isin(exclude_tokens) & df.mean_pol.notnull()]\n",
    "                                dflist[category_entity][vocab_type_entity][induction_method_type_entity][\"positive\"] = df.head(30)\n",
    "                                dflist[category_entity][vocab_type_entity][induction_method_type_entity][\"negative\"] = df.tail(30)\n",
    "                                del df\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing positive negative words\n",
      "bars\n",
      "shopping\n",
      "municipal\n",
      "food\n",
      "leisure\n",
      "sports\n",
      "planning\n",
      "health\n",
      "learn\n",
      "services\n",
      "grooming\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing positive negative words\")\n",
    "save_in_dir = SAVE_TOP30_POS_NEG_WORDS_ALL_MODELS_DIR\n",
    "if not os.path.isdir(save_in_dir):\n",
    "    os.makedirs(save_in_dir)\n",
    "dfall = pd.DataFrame()\n",
    "posnegwordslist = {}\n",
    "for category in dflist:\n",
    "    print(category)\n",
    "#     posnegwordslist[category] = {}\n",
    "    try:\n",
    "        for vocab in dflist[category]:\n",
    "#             posnegwordslist[category][vocab] = {}\n",
    "            dfallpos = pd.DataFrame()\n",
    "            dfallneg = pd.DataFrame()\n",
    "            for method in dflist[category][vocab]:\n",
    "    #             print(\"index {}|{}|{}\".format(category,vocab.replace(\"vocab_top\",\"tp\"),method))\n",
    "    #             print(dflist[category][vocab][method][\"positive\"].token.values)\n",
    "                dfallpos[method] = dflist[category][vocab][method][\"positive\"].token.values\n",
    "                dfallneg[method] = dflist[category][vocab][method][\"negative\"].token.values\n",
    "            loc_dir = os.path.join(save_in_dir,category,vocab)\n",
    "            if not (os.path.exists(loc_dir) and os.path.isdir(loc_dir)):\n",
    "                os.makedirs(loc_dir)\n",
    "            dfallpos.to_csv(os.path.join(save_in_dir,category,vocab,\"{cat}_{voc}_top30_pos_words.csv\".format(cat=category,voc=vocab)))\n",
    "            dfallneg.to_csv(os.path.join(save_in_dir,category,vocab,\"{cat}_{voc}_top30_neg_words.csv\".format(cat=category,voc=vocab)))\n",
    "#             posnegwordslist[category][vocab][\"pos\"] = dfallpos\n",
    "#             posnegwordslist[category][vocab][\"neg\"] = dfallneg\n",
    "    except:\n",
    "        pass\n",
    "#         posnegwordslist.pop(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/condaenv/anaconda2/envs/sentimentInduction/socialsentRun/Top30PosNegWords\n"
     ]
    }
   ],
   "source": [
    "print(SAVE_TOP30_POS_NEG_WORDS_ALL_MODELS_DIR)\n",
    "save_in_dir = SAVE_TOP30_POS_NEG_WORDS_ALL_MODELS_DIR\n",
    "if not os.path.isdir(save_in_dir):\n",
    "    os.makedirs(save_in_dir)\n",
    "posnegwordslist['bars']['vocab_top5000']['pos'].to_csv(os.path.join(save_in_dir,\"bars_v5000_pos_words.csv\"))\n",
    "posnegwordslist['bars']['vocab_top5000']['neg'].to_csv(os.path.join(save_in_dir,\"bars_v5000_neg_words.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}